<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Atharva Sastikar - Portfolio</title>
  <link rel="stylesheet" href="styles.css" />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet" />
</head>
<body>

  <header>
    <h1>Atharva Sastikar</h1>
    <p>Data Engineer | ML & NLP Enthusiast | Cloud & ETL Specialist</p>
  </header>

  <main>
    <section class="about">
      <h2>About Me</h2>
      <p>
        Passionate about data engineering and building scalable ETL pipelines. Experienced with cloud data ecosystems and applying NLP & ML to solve real-world problems. Welcome to my portfolio showcasing some of my projects!
      </p>
    </section>

    <section class="projects">
      <h2>Projects</h2>

      <div class="project-card">
        <h3>ETL Pipeline using Airflow for House Data Analysis</h3>
        
        <p><strong>Problem Statement:</strong>  
        Real estate companies and analysts often face challenges processing large volumes of housing data efficiently and accurately. Manual data workflows are time-consuming and prone to errors, and there's a growing need for scalable, automated solutions to clean and manage data for actionable insights.</p>
        
        <p><strong>Technologies & Tools Used:</strong>  
        Apache Airflow, AWS S3, Python, Pandas, AWS QuickSight, Docker, SQL</p>
        
        <p><strong>Project Summary:</strong>  
        This project builds an automated ETL pipeline orchestrated by Apache Airflow, which extracts raw house data from AWS S3, applies data cleaning and transformation using Python and Pandas, and loads the processed data back into S3. The pipeline supports scheduled execution and error handling, ensuring data consistency and freshness. The cleansed data is then used for visualization and dashboarding in AWS QuickSight, enabling stakeholders to derive meaningful business insights through interactive reports. This setup demonstrates the integration of cloud storage, data processing, orchestration, and visualization services to build end-to-end data solutions in a scalable and maintainable manner.</p>
      </div>

      <div class="project-card">
        <h3>YouTube Transcript Summarizer</h3>
        
        <p><strong>Problem Statement:</strong>  
        With the exponential growth of video content, viewers struggle to quickly absorb key information from long videos. Automated summarization of video transcripts can enhance content accessibility and save time, but requires sophisticated NLP models to preserve context and meaning.</p>
        
        <p><strong>Technologies & Tools Used:</strong>  
        Python, YouTube Data API, Large Language Models (LLMs), Hugging Face Transformers, Facebook's BART model, Flask (optional)</p>
        
        <p><strong>Project Summary:</strong>  
        This project implements a YouTube transcript summarizer using state-of-the-art Large Language Models (LLMs). It fetches video transcripts via the YouTube Data API and leverages Facebook's BART text-to-text transformer model to generate concise and coherent summaries. The pipeline integrates powerful NLP techniques for preprocessing, including tokenization and sentence ranking, to improve summary quality and relevance. Optionally, the summarizer is deployed as a web app using Flask, providing users a seamless interface to input video URLs and receive quick summaries. This project highlights practical skills in API integration, transformer-based NLP, and building AI-powered applications for enhancing digital content consumption.</p>
      </div>

    </section>
  </main>

  <footer>
    <p>Â© 2025 Atharva Sastikar | <a href="mailto:sastikaratharva55@gmail.com">Contact Me</a></p>
  </footer>

</body>
</html>
